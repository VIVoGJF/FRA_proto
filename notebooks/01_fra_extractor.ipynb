{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f0b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1 — Imports & Paths\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "try:\n",
    "    import camelot\n",
    "    _HAS_CAMELOT = True\n",
    "except Exception:\n",
    "    _HAS_CAMELOT = False\n",
    "    print(\"Camelot not installed or Ghostscript missing. Please install it.\")\n",
    "\n",
    "BASE_DIR = Path.cwd().parent  # assumes notebooks/ inside prototype/\n",
    "RAW_DOCS = BASE_DIR / \"data\" / \"raw\" / \"documents\"\n",
    "OUTPUT_EXCEL = BASE_DIR / \"data\" / \"processed\" / \"excel\"\n",
    "OUTPUT_MERGED = BASE_DIR / \"data\" / \"processed\" / \"text\"\n",
    "MAPPING_FILE = BASE_DIR / \"data\" / \"external\" / \"district_mapping.csv\"\n",
    "\n",
    "OUTPUT_EXCEL.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_MERGED.mkdir(parents=True, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3153e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_df = pd.read_csv(MAPPING_FILE, dtype=str).fillna(\"\")\n",
    "mapping_df.columns = [c.strip().lower() for c in mapping_df.columns]\n",
    "\n",
    "def norm_text(s):\n",
    "    s = (s or \"\")\n",
    "    s = s.strip().lower()\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "for col in [\"block\", \"village\", \"district\", \"state\"]:\n",
    "    if col in mapping_df.columns:\n",
    "        mapping_df[col + \"_norm\"] = mapping_df[col].apply(norm_text)\n",
    "    else:\n",
    "        mapping_df[col + \"_norm\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66e315c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_to_excel(pdf_path: Path, out_dir: Path) -> Path:\n",
    "    \"\"\"Extract tables from PDF into Excel using Camelot, try both flavors.\"\"\"\n",
    "    out_path = out_dir / f\"{pdf_path.stem}.xlsx\"\n",
    "    if out_path.exists():\n",
    "        print(f\"⏩ Skipping {pdf_path.name}, Excel already exists.\")\n",
    "        return out_path\n",
    "\n",
    "    if not _HAS_CAMELOT:\n",
    "        raise RuntimeError(\"Camelot not available\")\n",
    "\n",
    "    # First try lattice\n",
    "    tables = camelot.read_pdf(str(pdf_path), pages=\"all\", flavor=\"lattice\")\n",
    "\n",
    "    # If lattice fails or produces too few columns, try stream\n",
    "    if not tables or max(len(t.df.columns) for t in tables) < 3:\n",
    "        tables = camelot.read_pdf(str(pdf_path), pages=\"all\", flavor=\"stream\")\n",
    "\n",
    "    if not tables:\n",
    "        print(f\"❌ No tables found in {pdf_path.name}\")\n",
    "        return None\n",
    "\n",
    "    dfs = [t.df for t in tables]\n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    df_all.to_excel(out_path, index=False)\n",
    "    print(f\"✅ Extracted {len(df_all)} rows from {pdf_path.name} → {out_path.name}\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "721bc8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:23: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:23: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\Vivek\\AppData\\Local\\Temp\\ipykernel_21404\\1465625069.py:23: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  \"sl\\.? no|name of|format for submission|district\", case=False, na=False\n"
     ]
    }
   ],
   "source": [
    "def clean_excel(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Clean an extracted Excel into block | village | beneficiary.\"\"\"\n",
    "    df = pd.read_excel(file_path, dtype=str).fillna(\"\")\n",
    "    df.columns = [str(c).strip().lower() for c in df.columns]\n",
    "\n",
    "    # Detect columns\n",
    "    block_col = next((c for c in df.columns if \"block\" in c), None)\n",
    "    village_col = next((c for c in df.columns if \"village\" in c), None)\n",
    "    benef_col = next((c for c in df.columns if any(k in c for k in [\"beneficiary\", \"patta\", \"holder\", \"name\"])), None)\n",
    "\n",
    "    block_col = block_col or df.columns[-2] if len(df.columns) > 1 else df.columns[0]\n",
    "    village_col = village_col or df.columns[-1] if len(df.columns) > 1 else \"\"\n",
    "    benef_col = benef_col or df.columns[0]\n",
    "\n",
    "    df_clean = pd.DataFrame({\n",
    "        \"block\": df[block_col].astype(str).str.strip(),\n",
    "        \"village\": df[village_col].astype(str).str.strip() if village_col else \"\",\n",
    "        \"beneficiary\": df[benef_col].astype(str).str.strip()\n",
    "    })\n",
    "\n",
    "    # Drop junk rows (headers, footers, repeated \"Name of the District\" etc.)\n",
    "    df_clean = df_clean[~df_clean[\"beneficiary\"].str.contains(\n",
    "        \"sl\\.? no|name of|format for submission|district\", case=False, na=False\n",
    "    )]\n",
    "\n",
    "    # Remove empty rows\n",
    "    df_clean = df_clean[(df_clean[\"beneficiary\"] != \"\") & (df_clean[\"beneficiary\"].str.lower() != \"nan\")]\n",
    "\n",
    "    # Deduplicate\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e197e2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 2926 rows from AngulFRABenefLists.pdf → AngulFRABenefLists.xlsx\n",
      "✅ Extracted 2468 rows from BalasoreFRABeneficiaries.pdf → BalasoreFRABeneficiaries.xlsx\n",
      "✅ Extracted 6033 rows from GanjamFRABeneficiariesList_2.pdf → GanjamFRABeneficiariesList_2.xlsx\n"
     ]
    }
   ],
   "source": [
    "all_dfs = []\n",
    "\n",
    "# Step 1: Extract PDF → Excel (skip if already exists)\n",
    "for pdf in RAW_DOCS.glob(\"*.pdf\"):\n",
    "    excel_path = extract_pdf_to_excel(pdf, OUTPUT_EXCEL)\n",
    "    if excel_path:\n",
    "        df_norm = clean_excel(excel_path)\n",
    "        all_dfs.append(df_norm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37c79f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block — Extract block (col 1), village (col 3), beneficiary (col 4)\n",
    "def extract(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract only Block (col 1), Village (col 3), Beneficiary (col 4)\n",
    "    from a fixed-structure Excel file.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path, header=None, dtype=str).fillna(\"\")\n",
    "\n",
    "    # Extract required columns\n",
    "    df_clean = pd.DataFrame({\n",
    "        \"block\": df.iloc[:, 1].str.strip(),\n",
    "        \"village\": df.iloc[:, 3].str.strip(),\n",
    "        \"beneficiary\": df.iloc[:, 4].str.strip()\n",
    "    })\n",
    "\n",
    "    # Remove junk rows (headers, repeated labels, numbers only, etc.)\n",
    "    mask = (\n",
    "        df_clean.apply(lambda row: any(\n",
    "            re.search(r\"(sl\\.? no|name of|format for submission|district|taluk)\", str(x), re.I)\n",
    "            for x in row.values\n",
    "        ), axis=1)\n",
    "        | df_clean.apply(lambda row: all(str(x).isdigit() or str(x).lower() in [\"nan\", \"\"] for x in row.values), axis=1)\n",
    "    )\n",
    "    df_clean = df_clean[~mask]\n",
    "\n",
    "    # Deduplicate\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "\n",
    "    return df_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fd0f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_village_beneficiary(df):\n",
    "    \"\"\"Split rows where village+beneficiary got merged into one cell (no mapping needed).\"\"\"\n",
    "    fixed_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        village_val = str(row[\"village\"]).strip()\n",
    "        benef_val = str(row[\"beneficiary\"]).strip()\n",
    "\n",
    "        # Case 1: Beneficiary missing, village has multiple words\n",
    "        if not benef_val and len(village_val.split()) > 1:\n",
    "            parts = village_val.split(maxsplit=1)\n",
    "            row[\"village\"] = parts[0]\n",
    "            row[\"beneficiary\"] = parts[1] if len(parts) > 1 else \"\"\n",
    "\n",
    "        # Case 2: Both filled, but village has multiple words\n",
    "        elif benef_val and len(village_val.split()) > 1:\n",
    "            parts = village_val.split(maxsplit=1)\n",
    "            row[\"village\"] = parts[0]\n",
    "            row[\"beneficiary\"] = (parts[1] + \" \" + benef_val).strip() if len(parts) > 1 else benef_val\n",
    "\n",
    "        fixed_rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(fixed_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0cafac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_match_block(block_name, mapping_blocks, threshold=75):\n",
    "    \"\"\"\n",
    "    Find the closest block from mapping_blocks using fuzzy string match.\n",
    "    Returns best match if similarity >= threshold, else None.\n",
    "    \"\"\"\n",
    "    if not block_name or str(block_name).lower() in [\"nan\", \"\"]:\n",
    "        return None\n",
    "    \n",
    "    match, score, _ = process.extractOne(\n",
    "        block_name, \n",
    "        mapping_blocks, \n",
    "        scorer=fuzz.token_sort_ratio\n",
    "    )\n",
    "    return match if score >= threshold else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc6a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_state_district_fuzzy(df, mapping_df, threshold=85):\n",
    "    df = df.copy()\n",
    "    df[\"block_norm\"] = df[\"block\"].apply(norm_text)\n",
    "\n",
    "    # Deduplicate mapping_df by block_norm (keep first)\n",
    "    mapping_df = (\n",
    "        mapping_df.assign(block_norm=mapping_df[\"block\"].apply(norm_text))\n",
    "        .drop_duplicates(subset=[\"block_norm\"])\n",
    "    )\n",
    "\n",
    "    # Create dictionary\n",
    "    mapping_dict = mapping_df.set_index(\"block_norm\")[[\"state\", \"district\"]].to_dict(orient=\"index\")\n",
    "\n",
    "    mapping_blocks = list(mapping_dict.keys())\n",
    "\n",
    "    state_list, district_list = [], []\n",
    "\n",
    "    for block in df[\"block_norm\"]:\n",
    "        if block in mapping_dict:  # exact match\n",
    "            state_list.append(mapping_dict[block][\"state\"])\n",
    "            district_list.append(mapping_dict[block][\"district\"])\n",
    "        else:  # fuzzy match\n",
    "            best = fuzzy_match_block(block, mapping_blocks, threshold)\n",
    "            if best:\n",
    "                state_list.append(mapping_dict[best][\"state\"])\n",
    "                district_list.append(mapping_dict[best][\"district\"])\n",
    "            else:\n",
    "                state_list.append(None)\n",
    "                district_list.append(None)\n",
    "\n",
    "    df[\"state\"] = state_list\n",
    "    df[\"district\"] = district_list\n",
    "\n",
    "    return df[[\"state\", \"district\", \"block\", \"village\", \"beneficiary\"]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59c8c395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final rows with fuzzy state+district mapping: 10621\n"
     ]
    }
   ],
   "source": [
    "all_clean = []\n",
    "for excel in OUTPUT_EXCEL.glob(\"*.xlsx\"):\n",
    "    clean_df = extract(excel)\n",
    "    clean_df = fix_village_beneficiary(clean_df)\n",
    "    clean_df = add_state_district_fuzzy(clean_df, mapping_df, threshold=85)  # 👈 fuzzy backtracking\n",
    "    all_clean.append(clean_df)\n",
    "\n",
    "merged_clean = pd.concat(all_clean, ignore_index=True)\n",
    "\n",
    "# Save outputs\n",
    "merged_clean.to_csv(OUTPUT_MERGED / \"fra.csv\", index=False)\n",
    "\n",
    "print(\"✅ Final rows with fuzzy state+district mapping:\", len(merged_clean))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
